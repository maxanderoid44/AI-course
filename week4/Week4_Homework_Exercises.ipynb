{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Homework Exercises\n",
    "## Advanced Machine Learning & Algorithm Implementation\n",
    "\n",
    "**Duration:** 4-6 hours\n",
    "**Prerequisites:** Completion of Week 4 Lab Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Implement advanced search algorithms and graph traversal\n",
    "2. Build and compare multiple ML models for spam detection\n",
    "3. Perform feature engineering and model optimization\n",
    "4. Create a complete ML pipeline with cross-validation\n",
    "5. Analyze model performance and interpret results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Advanced Search Algorithms (60 minutes)\n",
    "\n",
    "### Task 1.1: Implement A* Search Algorithm\n",
    "\n",
    "**Instructions:** Implement the A* search algorithm for finding the shortest path in a weighted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample weighted graph\n",
    "graph = {\n",
    "    'A': [('B', 4), ('C', 2)],\n",
    "    'B': [('A', 4), ('D', 3), ('E', 1)],\n",
    "    'C': [('A', 2), ('D', 1), ('F', 5)],\n",
    "    'D': [('B', 3), ('C', 1), ('E', 2), ('F', 3)],\n",
    "    'E': [('B', 1), ('D', 2), ('F', 4)],\n",
    "    'F': [('C', 5), ('D', 3), ('E', 4)]\n",
    "}\n",
    "\n",
    "# Heuristic function (straight-line distance to goal)\n",
    "heuristic = {\n",
    "    'A': 6, 'B': 4, 'C': 4, 'D': 2, 'E': 3, 'F': 0\n",
    "}\n",
    "\n",
    "def a_star_search(graph, start, goal, heuristic):\n",
    "    \"\"\"\n",
    "    TODO: Implement A* search algorithm\n",
    "    Args:\n",
    "        graph: weighted graph as adjacency list\n",
    "        start: starting node\n",
    "        goal: goal node\n",
    "        heuristic: heuristic function for each node\n",
    "    Returns:\n",
    "        path: list of nodes from start to goal\n",
    "        cost: total cost of the path\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "path, cost = a_star_search(graph, 'A', 'F', heuristic)\n",
    "print(f\"A* Path: {path}\")\n",
    "print(f\"Total Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Compare Search Algorithms\n",
    "\n",
    "**Instructions:** Implement a function to compare BFS, DFS, and A* search algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_search_algorithms(graph, start, goal):\n",
    "    \"\"\"\n",
    "    TODO: Compare BFS, DFS, and A* search algorithms\n",
    "    - Measure execution time\n",
    "    - Count nodes explored\n",
    "    - Find path length\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Implement BFS comparison\n",
    "    \n",
    "    # TODO: Implement DFS comparison\n",
    "    \n",
    "    # TODO: Implement A* comparison\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test comparison\n",
    "comparison_results = compare_search_algorithms(graph, 'A', 'F')\n",
    "print(\"Algorithm Comparison:\")\n",
    "for algo, metrics in comparison_results.items():\n",
    "    print(f\"\\n{algo}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Advanced Email Spam Detection (120 minutes)\n",
    "\n",
    "### Task 2.1: Multiple Model Comparison\n",
    "\n",
    "**Instructions:** Implement and compare multiple ML models for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data (reuse from lab)\n",
    "df = pd.read_csv(\"mail_data.csv\")\n",
    "data = df.where((pd.notnull(df)), '')\n",
    "data.loc[data['Category'] == 'spam', 'Category'] = 1\n",
    "data.loc[data['Category'] == 'ham', 'Category'] = 0\n",
    "\n",
    "X = data['Message']\n",
    "y = data['Category']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Data prepared successfully!\")\n",
    "print(f\"Training set: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing set: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', random_state=42),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# TODO: Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # TODO: Train model\n",
    "    \n",
    "    # TODO: Make predictions\n",
    "    \n",
    "    # TODO: Calculate metrics\n",
    "    \n",
    "    # TODO: Store results\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# TODO: Create comparison table\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Feature Engineering\n",
    "\n",
    "**Instructions:** Create additional features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    TODO: Extract additional features from text\n",
    "    - Text length\n",
    "    - Word count\n",
    "    - Number of exclamation marks\n",
    "    - Number of capital letters\n",
    "    - Presence of spam keywords\n",
    "    - Presence of URLs\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # TODO: Implement feature extraction\n",
    "    \n",
    "    return features\n",
    "\n",
    "# TODO: Apply feature extraction to dataset\n",
    "def create_enhanced_features(texts):\n",
    "    \"\"\"Create enhanced feature matrix\"\"\"\n",
    "    enhanced_features = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # TODO: Extract features for each text\n",
    "        pass\n",
    "    \n",
    "    return pd.DataFrame(enhanced_features)\n",
    "\n",
    "# Create enhanced features\n",
    "enhanced_train = create_enhanced_features(X_train)\n",
    "enhanced_test = create_enhanced_features(X_test)\n",
    "\n",
    "print(\"Enhanced features created!\")\n",
    "print(f\"Training features: {enhanced_train.shape}\")\n",
    "print(f\"Testing features: {enhanced_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Combine TF-IDF and enhanced features\n",
    "# Hint: Use hstack or concatenate\n",
    "\n",
    "# TODO: Train model with combined features\n",
    "\n",
    "# TODO: Compare performance with original model\n",
    "\n",
    "print(\"Feature engineering results:\")\n",
    "print(f\"Original accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Enhanced accuracy: {enhanced_accuracy:.4f}\")\n",
    "print(f\"Improvement: {enhanced_accuracy - original_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Cross-Validation and Hyperparameter Tuning\n",
    "\n",
    "**Instructions:** Implement cross-validation and hyperparameter tuning for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TODO: Create pipeline with vectorizer and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# TODO: Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'vectorizer__max_features': [3000, 5000, 7000],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "# TODO: Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# TODO: Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test set accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Model Analysis and Interpretation (60 minutes)\n",
    "\n",
    "### Task 3.1: Feature Importance Analysis\n",
    "\n",
    "**Instructions:** Analyze and visualize feature importance for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get feature importance from the best model\n",
    "feature_names = \n",
    "feature_importance = \n",
    "\n",
    "# TODO: Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# TODO: Plot top 20 most important features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features for Spam Detection')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 spam indicators:\")\n",
    "print(top_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Error Analysis\n",
    "\n",
    "**Instructions:** Analyze misclassified emails to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get predictions from best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# TODO: Find misclassified examples\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified_indices)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_indices)/len(y_test):.4f}\")\n",
    "\n",
    "# TODO: Analyze some misclassified examples\n",
    "print(\"\\nSample misclassified emails:\")\n",
    "for i in misclassified_indices[:5]:\n",
    "    true_label = \"SPAM\" if y_test.iloc[i] == 1 else \"HAM\"\n",
    "    pred_label = \"SPAM\" if y_pred[i] == 1 else \"HAM\"\n",
    "    email = X_test.iloc[i][:100] + \"...\"\n",
    "    \n",
    "    print(f\"\\nTrue: {true_label}, Predicted: {pred_label}\")\n",
    "    print(f\"Email: {email}\")\n",
    "\n",
    "# TODO: Analyze patterns in misclassifications\n",
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Real-World Application (60 minutes)\n",
    "\n",
    "### Task 4.1: Create a Spam Detection API\n",
    "\n",
    "**Instructions:** Build a simple API for spam detection using Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# TODO: Save the best model\n",
    "joblib.dump(best_model, 'spam_detector_model.pkl')\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# TODO: Create Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# TODO: Load the model\n",
    "model = joblib.load('spam_detector_model.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_spam():\n",
    "    \"\"\"\n",
    "    TODO: Implement prediction endpoint\n",
    "    - Accept JSON with 'message' field\n",
    "    - Return prediction (spam/ham) and confidence\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({'status': 'healthy', 'model': 'spam_detector'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=5000)\n",
    "\n",
    "print(\"API created! Run the cell below to test it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the API\n",
    "import requests\n",
    "\n",
    "# Test messages\n",
    "test_messages = [\n",
    "    \"Meeting tomorrow at 3 PM\",\n",
    "    \"FREE MONEY NOW! CLICK HERE!\",\n",
    "    \"Please review the quarterly report\"\n",
    "]\n",
    "\n",
    "for message in test_messages:\n",
    "    # TODO: Make API request\n",
    "    response = requests.post('http://localhost:5000/predict', \n",
    "                            json={'message': message})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Message: {message[:30]}...\")\n",
    "        print(f\"Prediction: {result['prediction']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\\n\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Documentation and Report (30 minutes)\n",
    "\n",
    "### Task 5.1: Create Project Documentation\n",
    "\n",
    "**Instructions:** Create comprehensive documentation for your spam detection project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create project documentation\n",
    "documentation = \"\"\"\n",
    "# Email Spam Detection Project Report\n",
    "\n",
    "## Project Overview\n",
    "This project implements a machine learning system for email spam detection using various algorithms and feature engineering techniques.\n",
    "\n",
    "## Dataset\n",
    "- Source: mail_data.csv\n",
    "- Size: {dataset_size} emails\n",
    "- Classes: Spam ({spam_count}), Ham ({ham_count})\n",
    "\n",
    "## Models Evaluated\n",
    "{model_comparison}\n",
    "\n",
    "## Best Model\n",
    "- Algorithm: {best_model_name}\n",
    "- Accuracy: {best_accuracy:.4f}\n",
    "- F1 Score: {best_f1:.4f}\n",
    "- Parameters: {best_params}\n",
    "\n",
    "## Feature Engineering\n",
    "- TF-IDF vectorization\n",
    "- Text length analysis\n",
    "- Spam keyword detection\n",
    "- URL detection\n",
    "\n",
    "## Key Findings\n",
    "1. {finding_1}\n",
    "2. {finding_2}\n",
    "3. {finding_3}\n",
    "\n",
    "## Future Improvements\n",
    "1. {improvement_1}\n",
    "2. {improvement_2}\n",
    "3. {improvement_3}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Fill in the documentation with actual values\n",
    "documentation = documentation.format(\n",
    "    dataset_size=len(data),\n",
    "    spam_count=len(data[data['Category'] == 1]),\n",
    "    ham_count=len(data[data['Category'] == 0]),\n",
    "    model_comparison=results_df.to_string(),\n",
    "    best_model_name=\"Random Forest\",\n",
    "    best_accuracy=test_score,\n",
    "    best_f1=grid_search.best_score_,\n",
    "    best_params=str(grid_search.best_params_),\n",
    "    finding_1=\"Random Forest performed best with feature engineering\",\n",
    "    finding_2=\"TF-IDF features are crucial for text classification\",\n",
    "    finding_3=\"Cross-validation helps prevent overfitting\",\n",
    "    improvement_1=\"Try deep learning models (LSTM, BERT)\",\n",
    "    improvement_2=\"Collect more diverse training data\",\n",
    "    improvement_3=\"Implement ensemble methods\"\n",
    ")\n",
    "\n",
    "# Save documentation\n",
    "with open('spam_detection_report.md', 'w') as f:\n",
    "    f.write(documentation)\n",
    "\n",
    "print(\"Documentation saved as 'spam_detection_report.md'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Completion Checklist\n",
    "\n",
    "- [ ] Implemented A* search algorithm\n",
    "- [ ] Compared BFS, DFS, and A* algorithms\n",
    "- [ ] Built multiple ML models (Logistic Regression, Random Forest, SVM, Naive Bayes)\n",
    "- [ ] Implemented feature engineering techniques\n",
    "- [ ] Performed cross-validation and hyperparameter tuning\n",
    "- [ ] Analyzed feature importance\n",
    "- [ ] Conducted error analysis on misclassified emails\n",
    "- [ ] Created a Flask API for spam detection\n",
    "- [ ] Generated comprehensive project documentation\n",
    "- [ ] All code runs without errors\n",
    "\n",
    "**Total Time:** ~4-6 hours\n",
    "**Submission:** Upload your completed notebook and generated files\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus Challenges (Optional)\n",
    "\n",
    "1. **Deep Learning:** Implement an LSTM or BERT model for spam detection\n",
    "2. **Real-time Processing:** Create a real-time email processing system\n",
    "3. **Multi-language Support:** Extend the model to handle multiple languages\n",
    "4. **Web Interface:** Build a web interface using Streamlit or Flask\n",
    "5. **Deployment:** Deploy your model to a cloud platform (AWS, Google Cloud, etc.)\n",
    "\n",
    "**Note:** Bonus challenges can earn extra credit and demonstrate advanced skills!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
